{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iris dataset analysis.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "kHpo7LluNd1x",
        "y14UDfVNNhC1",
        "IeyuEAsY0woy",
        "UmURY0Lq1E_F",
        "kadIhMX9hUW1",
        "VMCDsqPtiaPC",
        "F9gk3dIKuMXd",
        "3Kccg6Pol4Tc",
        "68VucxaZuhC3",
        "dLQUw4_cl_53",
        "x0_6VXuEbFPb",
        "ZpnKlQJEb6TA",
        "sgNOplcGyoV2",
        "J1Sw4EmZcTQp",
        "MCPc5WmBq3--"
      ],
      "authorship_tag": "ABX9TyOe9ZiN78mrVDSCkW12YOiN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/badalahmmed/Data-science/blob/master/iris_dataset_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps in Applied Machine Learning & Data Science :\n",
        " 1. Load Library\n",
        " 2. Load Dataset to which Machine Learning Algorithm to be applied\n",
        "    Either\n",
        "     1. load from a CSV file or\n",
        "     2. load from a Database   \n",
        " 3. Summarisation of Data to understand dataset (Descriptive Statistics) or Preliminary Analysis\n",
        " 4. Visualisation of Data to understand dataset (Plots, Graphs etc.)\n",
        " 5. Data pre-processing & Data transformation (split into train-test datasets)\n",
        " 6. Application of a Machine Learning Algorithm to training dataset \n",
        "   1. setup a ML algorithm and parameter settings\n",
        "   2. HPO using grid, baysian and, cross validation with training dataset\n",
        "   3. training & fitting Algorithm with training Dataset\n",
        "   4. evaluation of trained Algorithm (or Model) and result\n",
        "   5. saving the trained model for future prediction\n",
        " 7. Load the saved model and apply it to new dataset for prediction"
      ],
      "metadata": {
        "id": "HHk1-eoXs92q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Library modules"
      ],
      "metadata": {
        "id": "kHpo7LluNd1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-plot\n"
      ],
      "metadata": {
        "id": "sHyK_rWfwvWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-optimize"
      ],
      "metadata": {
        "id": "Jr3BuFw9KLTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sklearn"
      ],
      "metadata": {
        "id": "LkxJE8y1w7iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle as pk\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import scikitplot as skplt\n",
        "from   sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV,RepeatedStratifiedKFold\n",
        "from   sklearn.ensemble import RandomForestClassifier\n",
        "from   sklearn.metrics import accuracy_score, classification_report, cohen_kappa_score, confusion_matrix"
      ],
      "metadata": {
        "id": "cZegyDr-NgPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Dataset"
      ],
      "metadata": {
        "id": "y14UDfVNNhC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mount Google Drive and set the paths of datasets\n"
      ],
      "metadata": {
        "id": "IeyuEAsY0woy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.insert(0, '/content/drive/MyDrive/Random Forest Classifiers/Iris dataset analysis/Dataset')"
      ],
      "metadata": {
        "id": "Pky5PJPb0wJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###function for csv file loading"
      ],
      "metadata": {
        "id": "UmURY0Lq1E_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_csv_dataset_withoutFeatureName(filename):\n",
        "        #define the column Names\n",
        "        col_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'Species']\n",
        "        #read the csv data\n",
        "        dataset = pd.read_csv(filename, sep = ',', names = col_names)\n",
        "        #print data shape, some data and column names\n",
        "        print(\"Data Shape\", dataset.shape)\n",
        "        print(\"...............................................\")\n",
        "        print(\"Datasets's Head\", dataset.head(5)) \n",
        "        print(\"...............................................\")\n",
        "        print(\"Column Names\",dataset.columns)\n",
        "        print(\"...............................................\")\n",
        "        feature_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
        "        target = 'Species'\n",
        "        return feature_names, target, dataset"
      ],
      "metadata": {
        "id": "E4AnRPZGhTs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_csv_dataset(filename):\n",
        "  dataset = pd.read_csv(path, sep = ',')\n",
        "  #print data shape, some data and column names\n",
        "  print(\"Data Shape\", dataset.shape)\n",
        "  print(\"...............................................\")\n",
        "  print( dataset.head(5)) \n",
        "  print(\"...............................................\")\n",
        "  print();print(\"Column Names\",dataset.columns)\n",
        "  print(\"...............................................\")\n",
        "  feature_names = dataset.columns[:4]\n",
        "  target = dataset.columns[-1:][0]\n",
        "  return feature_names, target, dataset\n"
      ],
      "metadata": {
        "id": "ciQKUabmCE5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path=\"/content/drive/MyDrive/Random Forest Classifiers/Iris dataset analysis/Dataset/Iris.csv\""
      ],
      "metadata": {
        "id": "SE7qujPuzvbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names, target, dataset =load_csv_dataset(path)"
      ],
      "metadata": {
        "id": "x-NI7e2nBIfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preliminary data analysis\n"
      ],
      "metadata": {
        "id": "kadIhMX9hUW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Information About Missing Values\n",
        "2. Information about Feature Variables\n",
        "3. Correlation\n",
        "3. Ranking of Correlation Coefficients\n",
        "4. Highly correlated variables (Absolute Correlations)\n",
        "5. Information about the target variable"
      ],
      "metadata": {
        "id": "qMvihgjfhcod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# Helper modules for Descriptive Statistics\n",
        "# -------------------------------------------------------------------------    \n",
        "def get_redundant_pairs(df):\n",
        "        pairs_to_drop = set()\n",
        "        cols = df.columns\n",
        "        for i in range(0, df.shape[1]):\n",
        "            for j in range(0, i+1):\n",
        "                pairs_to_drop.add((cols[i], cols[j]))\n",
        "        return pairs_to_drop\n",
        "\n",
        "def get_top_abs_correlations(df, n=5): \n",
        "        au_corr = df.corr().unstack()\n",
        "        labels_to_drop = get_redundant_pairs(df)\n",
        "        au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
        "        return au_corr[0:n]\n",
        "\n",
        "def corrank(X):\n",
        "        import itertools\n",
        "        df = pd.DataFrame([[(i,j), \n",
        "                   X.corr().loc[i,j]] for i,j in list(itertools.combinations(X.corr(), 2))],\n",
        "                   columns=['pairs','corr'])\n",
        "        print(df.sort_values(by='corr',ascending=False))\n",
        "        print()"
      ],
      "metadata": {
        "id": "T2sY4GY_dh6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "#  function of descriptive statistics and correlation matrix \n",
        "# -------------------------------------------------------------------------    \n",
        "def data_descriptiveStats(feature_names, target, dataset):\n",
        "        # Count Number of Missing Value on Each Column    \n",
        "        print(); print('Count Number of Missing Value on Each Column: ')        \n",
        "        print(); print(dataset[feature_names].isnull().sum(axis=0))\n",
        "        print(); print(\"Target Missing\",dataset[target].isnull().sum(axis=0))    \n",
        "        print(\"...............................................\")  \n",
        "        \n",
        "        # Get Information on the feature variables\n",
        "        print(); print('Get Information on the feature variables: ')            \n",
        "        print(); print(dataset[feature_names].info())\n",
        "        print(); print(dataset[feature_names].describe())\n",
        "        print(\"...............................................\")\n",
        "\n",
        "        # correlation\n",
        "        print();print(\"Corelation\")\n",
        "        pd.set_option('precision', 2)\n",
        "        print(); print(dataset[feature_names].corr(method ='pearson'))    \n",
        "        print(\"...............................................\")\n",
        "\n",
        "        # Ranking of Correlation Coefficients among Variable Pairs\n",
        "        print(); print(\"Ranking of Correlation Coefficients:\")    \n",
        "        corrank(dataset[feature_names])\n",
        "        print(\"...............................................\")\n",
        "\n",
        "        # Print Highly Correlated Variables\n",
        "        print(); print(\"Highly correlated variables (Absolute Correlations):\")\n",
        "        print(); print(get_top_abs_correlations(dataset[feature_names], 8))\n",
        "        print(\"...............................................\")\n",
        "\n",
        "        # Get Information on the target    \n",
        "        print(\"Target Descriptions\")\n",
        "        print(); print(dataset[target].describe())  \n",
        "        print(\"Target groups\")\n",
        "        print(); print(dataset.groupby(target).size())"
      ],
      "metadata": {
        "id": "j5B9tZjohbLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Visualization\n"
      ],
      "metadata": {
        "id": "VMCDsqPtiaPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Box plot of each Numerical Features\n",
        "2. Histogram plot of each Numerical Features\n",
        "3. Co-relation metrix of  Numerical Features\n",
        "4. Scatter Matrix Plot\n",
        "5. Pie chart for target"
      ],
      "metadata": {
        "id": "MUk8FBl1ilev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#boxplot function\n",
        "def Box_Plot(feature_names):\n",
        "  i = 1\n",
        "  print(); print('BOX plot of each numerical features')\n",
        "  plt.figure(figsize=(11,9)) \n",
        "  plt.suptitle('Box Plot') \n",
        "  for col in feature_names:\n",
        "      plt.subplot(2,2,i)\n",
        "      plt.title( col)\n",
        "      plt.axis('on')\n",
        "      plt.tick_params(axis='both', left=True, top=False, right=False, bottom=True, \n",
        "                      labelleft=False, labeltop=False, labelright=False, labelbottom=False)\n",
        "      dataset[col].plot(kind='box', subplots=True, sharex=False, sharey=False)\n",
        "      \n",
        "      \n",
        "      i += 1\n",
        "  plt.show() "
      ],
      "metadata": {
        "id": "Zds3RI6xiiI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#histogram function\n",
        "def Histogram_Plot(feature_names):\n",
        "  j = 1\n",
        "  print(); print('Histogram of each Numerical Feature')\n",
        "  plt.figure(figsize=(11,9))  \n",
        "  plt.suptitle('Box Plot')    \n",
        "  for col in feature_names:\n",
        "      plt.subplot(2,2,j)\n",
        "      plt.title( col)\n",
        "      plt.axis('on')\n",
        "      plt.tick_params(axis='both', left=True, top=False, right=False, bottom=False, \n",
        "                      labelleft=False, labeltop=False, labelright=False, labelbottom=False)\n",
        "      dataset[col].hist()\n",
        "      j += 1\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "udR4emNOwkF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correlation matrix\n",
        "def Corelation_Matrix_Plot(dataset):\n",
        "  print(); print('Heat of each Numerical Feature')\n",
        "  fig, ax = plt.subplots()\n",
        "  plt.title(\"correlation matrixplot or Heat Plot\")\n",
        "  corr = dataset.corr()\n",
        "  ax = sns.heatmap(\n",
        "      corr, \n",
        "      vmin=-1, vmax=1, center=0,\n",
        "      cmap=sns.diverging_palette(20, 220, n=200),\n",
        "      square=True\n",
        "  )\n",
        "  ax.set_xticklabels(\n",
        "      ax.get_xticklabels(),\n",
        "      rotation=45,\n",
        "      horizontalalignment='right'\n",
        "  );"
      ],
      "metadata": {
        "id": "3GmVDFpMxMKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Pair_Plot(dataset):\n",
        "  # PairPlot using seaborn\n",
        "  print(); print('Scatter Matrix Plot')\n",
        "  sns.pairplot(dataset, hue = 'Species')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "qGCyFDbv8eVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Pie_Chart(target):\n",
        "  # Pie chart for Categorical Variables\n",
        "  print(); print('PIE Chart of for Target: ')\n",
        "  plt.figure(figsize=(11,9)) \n",
        "  i = 1\n",
        "  for colName in [target]:\n",
        "      labels = []; sizes = [];\n",
        "      df = dataset.groupby(colName).size()\n",
        "      for key in df.keys():\n",
        "          labels.append(key)\n",
        "          sizes.append(df[key])\n",
        "      # Plot PIE Chart with %\n",
        "      plt.subplot(2,2,i)\n",
        "      plt.axis('on')\n",
        "      plt.tick_params(axis='both', left=False, top=False, right=False, bottom=False, \n",
        "                      labelleft=True, labeltop=True, labelright=False, labelbottom=False)        \n",
        "      plt.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True, startangle=140)\n",
        "      plt.axis('equal')\n",
        "      i += 1; plt.savefig('Piefig.pdf', format='pdf')\n",
        "  plt.show()    "
      ],
      "metadata": {
        "id": "c_JCJ6t-9Ey-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Data_Visualization(dataset, feature_names, target):\n",
        "  Box_Plot(feature_names)\n",
        "  Histogram_Plot(feature_names)\n",
        "  Corelation_Matrix_Plot(dataset)\n",
        "  Pair_Plot(dataset)\n",
        "  Pie_Chart(target)"
      ],
      "metadata": {
        "id": "9en93ssFuTWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Visualization(dataset, feature_names,target)"
      ],
      "metadata": {
        "id": "MqVGO1lt-E8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data pre-processing & Data transformation (split into train-test datasets)"
      ],
      "metadata": {
        "id": "F9gk3dIKuMXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. split dataset as train ans test\n",
        "2. Using PCA"
      ],
      "metadata": {
        "id": "dbLySxJBuloW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Split the dataset into train and test set"
      ],
      "metadata": {
        "id": "3Kccg6Pol4Tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_split(feature_names, target, dataset):\n",
        "        # Data Transform - Split train : test datasets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(dataset.loc[:, feature_names], \n",
        "                                                            dataset.loc[:, target], test_size=0.33)\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "X_train, X_test, y_train, y_test = data_split(feature_names, target, dataset)"
      ],
      "metadata": {
        "id": "yM-UQ_hfgSIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Apply PCA "
      ],
      "metadata": {
        "id": "hP0O15MpqHii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function for Standarize the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "def Standardarization(dataset, feature_names):\n",
        "  x = dataset.loc[:, feature_names].values\n",
        "  x=StandardScaler().fit_transform(x)\n",
        "  return x\n",
        "\n",
        "#Function for Dimention reduction\n",
        "def Dimention_Reduction(n_components, Standard_Dataset):\n",
        "  pca = PCA(n_components)\n",
        "  principalComponents = pca.fit_transform(Standard_Dataset)\n",
        "  principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
        "  return principalDf\n",
        "\n",
        "\n",
        "#Function for Visualize 2D projections\n",
        "def Visualiza2DProjection(DimentionReducedDf):\n",
        "  fig = plt.figure(figsize = (8,8))\n",
        "  ax = fig.add_subplot(1,1,1) \n",
        "  ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "  ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "  ax.set_title('2 Component PCA', fontsize = 20)\n",
        "  targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
        "  colors = ['r', 'g', 'b']\n",
        "  for target, color in zip(targets,colors):\n",
        "      indicesToKeep = DimentionReducedDf['Species'] == target\n",
        "      ax.scatter(DimentionReducedDf.loc[indicesToKeep, 'principal component 1']\n",
        "                , DimentionReducedDf.loc[indicesToKeep, 'principal component 2']\n",
        "                , c = color\n",
        "                , s = 50)\n",
        "  ax.legend(targets)\n",
        "  ax.grid()"
      ],
      "metadata": {
        "id": "pmykpgrSOgIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "StandardData = Standardarization(dataset, feature_names)\n",
        "principalDf=Dimention_Reduction(2,StandardData)\n",
        "finalDf = pd.concat([principalDf, dataset[['Species']]], axis = 1) #add targets or Species with DimentionReduced Data\n",
        "Visualiza2DProjection(finalDf)\n"
      ],
      "metadata": {
        "id": "SsM7vlR0QpOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Application of a Machine Learning Algorithm to training dataset\n"
      ],
      "metadata": {
        "id": "68VucxaZuhC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. setup a ML algorithm and parameter settings\n",
        "2. Hyperparameter tuning by grid search, Baysian Search and kfold cross validation and training model with best HP\n",
        "3. evaluation of trained Algorithm (or Model) and result\n"
      ],
      "metadata": {
        "id": "AOWp1oSOusKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train the model\n"
      ],
      "metadata": {
        "id": "dLQUw4_cl_53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_model(X_train, y_train):\n",
        "        model = RandomForestClassifier(n_estimators=100, criterion='gini', \n",
        "                                  max_depth=8, min_samples_split=2, \n",
        "                                  min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
        "                                  max_features=None, max_leaf_nodes=None, \n",
        "                                  min_impurity_decrease=0.0, \n",
        "                                  bootstrap=True, oob_score=False, \n",
        "                                  warm_start=False, class_weight=None)\n",
        "        return model"
      ],
      "metadata": {
        "id": "yH-3dOD7agtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = training_model(X_train, y_train)"
      ],
      "metadata": {
        "id": "O-ftMx4hbBuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "tYJOk0kMbMpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grid Search"
      ],
      "metadata": {
        "id": "x0_6VXuEbFPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GridSearch(model, X_train,y_train,parameters, kfold):\n",
        "  grid = GridSearchCV(estimator=model, param_grid = parameters, cv = kfold, \n",
        "                    verbose = 1, n_jobs = -1, refit = True)\n",
        "  grid.fit(X_train, y_train)\n",
        "\n",
        "  # Results from Grid Search\n",
        "  print(\"\\n========================================================\")\n",
        "  print(\" Results from Grid Search \" )\n",
        "  print(\"========================================================\")    \n",
        "  print(\"\\n The best estimator across ALL searched params:\\n\",\n",
        "      grid.best_estimator_)\n",
        "  print(\"\\n The best score across ALL searched params:\\n\",\n",
        "      grid.best_score_)\n",
        "  print(\"\\n The best parameters across ALL searched params:\\n\",\n",
        "      grid.best_params_)\n",
        "  print(\"\\n ========================================================\")\n",
        "\n",
        "  return(grid.best_estimator_)"
      ],
      "metadata": {
        "id": "Wxohyqgwf8m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###try to see the efficiencies of hpo search"
      ],
      "metadata": {
        "id": "9ndKHC5LqvRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Baysian Search"
      ],
      "metadata": {
        "id": "ZpnKlQJEb6TA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt import BayesSearchCV\n",
        "def BaysianSearch(model, X_train, y_train,parameters, kfold ):\n",
        "  # define the search\n",
        "  baysian = BayesSearchCV(estimator=model, search_spaces = parameters, n_jobs=-1, cv=kfold)\n",
        "  baysian.fit(X_train, y_train)\n",
        "  # report the best result\n",
        "  print(\"Baysian score\",baysian.best_score_)\n",
        "  print(\"Baysian Best param\", baysian.best_params_)\n",
        "  return(baysian.best_estimator_)"
      ],
      "metadata": {
        "id": "o1p5G0hSqnok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Use cross validation"
      ],
      "metadata": {
        "id": "sgNOplcGyoV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validatin_and_fitting(model, X_train, y_train, kfold):\n",
        "        cv_results = cross_val_score(model, X_train, y_train, cv = kfold, scoring = 'accuracy', \n",
        "                                 n_jobs = -1, verbose = 1)\n",
        "        # Cross Validation Results\n",
        "        print()\n",
        "        print(\"Cross Validation results: \", cv_results)\n",
        "        prt_string = \"CV Mean Accuracy: %f (Std: %f)\"% (cv_results.mean(), cv_results.std())\n",
        "        print(prt_string)\n",
        "        \n",
        "        # Final fitting of the Model\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "        print(); print('========================================================')\n",
        "        print(); print(model.get_params(deep = True))\n",
        "        print(); print('========================================================')        \n",
        "        \n",
        "        # plot learning Curves\n",
        "        skplt.estimators.plot_learning_curve(model, X_train, y_train, figsize=(8,6))\n",
        "        plt.show()\n",
        "        \n",
        "        return model"
      ],
      "metadata": {
        "id": "2kS2nkX8yDkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Apply all the search"
      ],
      "metadata": {
        "id": "J1Sw4EmZcTQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  #Define search Space\n",
        "  parameters = {'max_depth'     : [4,10,12,15],\n",
        "                        'criterion'     : ['gini', 'entropy'],\n",
        "                        'max_features'  : ['auto', 'sqrt', 'log2'],\n",
        "                        'n_estimators'  : [50,100, 250, 500]\n",
        "                        # Add more parameters here for tuning\n",
        "                        # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "                        }\n",
        "kfold = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)"
      ],
      "metadata": {
        "id": "C9clsZosci41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = cross_validatin_and_fitting(model, X_train, y_train,kfold)\n",
        "GridSearch(model, X_train,y_train,parameters, kfold)\n",
        "BaysianSearch(model, X_train, y_train,parameters, kfold)\n"
      ],
      "metadata": {
        "id": "-MU3gCGccShn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evalute methods"
      ],
      "metadata": {
        "id": "MCPc5WmBq3--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate techniques:\n",
        "1. Confusion matrix\n",
        "2. Accuracy\n",
        "3. Precision\n",
        "4. Recall\n",
        "5. Specificity\n",
        "6. F1 score\n",
        "7. Precision-Recall or PR curve\n",
        "8. ROC (Receiver Operating Characteristics) curve\n",
        "9. PR vs ROC curve.\n",
        "\n",
        "Some common terms to be clear with are:\n",
        "1. True positives (TP): Predicted positive and are actually positive.\n",
        "2. False positives (FP): Predicted positive and are actually negative.\n",
        "3. True negatives (TN): Predicted negative and are actually negative.\n",
        "4. False negatives (FN): Predicted negative and are actually positive."
      ],
      "metadata": {
        "id": "Oc6A5aDNif9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# Evaluate the skill of the Trained model\n",
        "# -----------------------------------------------\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "        pred_Class          = model.predict(X_test)\n",
        "        acc                 = accuracy_score(y_test, pred_Class)\n",
        "        classReport         = classification_report(y_test, pred_Class)\n",
        "        confMatrix          = confusion_matrix(y_test, pred_Class) \n",
        "        kappa_score         = cohen_kappa_score(y_test, pred_Class)         \n",
        "        \n",
        "        print(); print('Evaluation of the trained model: ')\n",
        "        print(); print('Accuracy : ', acc)\n",
        "        print(); print('Kappa Score : ', kappa_score)\n",
        "        print(); print('Confusion Matrix :\\n', confMatrix)\n",
        "        print(); print('Classification Report :\\n',classReport)\n",
        "\n",
        "        pred_proba = model.predict_proba(X_test)\n",
        "        \n",
        "        # Add more plots here using scikit-plot\n",
        "        # ROC curves\n",
        "        skplt.metrics.plot_roc(y_test,pred_proba,figsize=(8,6)); plt.show()\n",
        "\n",
        "        # Confusion matrix\n",
        "        skplt.metrics.plot_confusion_matrix(y_test,pred_Class,figsize=(8,6)); plt.show()        \n",
        "\n",
        "        # precision recall curve\n",
        "        skplt.metrics.plot_precision_recall(y_test, pred_proba, \n",
        "                title='Precision-Recall Curve', plot_micro=True, \n",
        "                classes_to_plot=None, ax=None, figsize=(8,6), \n",
        "                cmap='nipy_spectral', title_fontsize='large', \n",
        "                text_fontsize='medium'); plt.show()        \n",
        "        \n",
        "        # Add more ... ... ...\n",
        "        \n",
        "        return model\n",
        "\n",
        "model = evaluate_model(model, X_test, y_test)"
      ],
      "metadata": {
        "id": "jQQx3-yZxNWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-7BIVPZAMuYW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}